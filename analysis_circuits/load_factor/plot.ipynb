{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70dbd03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 termloadfactor files\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_initial.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter150000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter125000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter175000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter100000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter200000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter325000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter75000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter450000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter425000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter350000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter275000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter50000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter300000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter225000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter250000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter25000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter375000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Keys in 10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise_termloadfactor_iter400000.npz: ['load_factors', 'term_descriptions', 'metadata']\n",
      "Successfully processed data for 1 models\n",
      "Attempting to load training data from: /home/goring/OnlineSGD/results_MSP/Phi_1304_mon_810_leap_load_layer2/complex_leap_exp_20250414_003722/10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise.npz\n",
      "Successfully loaded training data from /home/goring/OnlineSGD/results_MSP/Phi_1304_mon_810_leap_load_layer2/complex_leap_exp_20250414_003722/10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise.npz: 450 epochs\n",
      "Saved plot to /home/goring/OnlineSGD/results_MSP/Phi_1304_mon_810_leap_load_layer2/complex_leap_exp_20250414_003722/load_factors_depth3_10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise.png\n",
      "Successfully processed 228 data points\n",
      "Attempting to load training data from: /home/goring/OnlineSGD/results_MSP/Phi_1304_mon_810_leap_load_layer2/complex_leap_exp_20250414_003722/10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise.npz\n",
      "Successfully loaded training data from /home/goring/OnlineSGD/results_MSP/Phi_1304_mon_810_leap_load_layer2/complex_leap_exp_20250414_003722/10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise.npz: 450 epochs\n",
      "Saved plot to /home/goring/OnlineSGD/results_MSP/Phi_1304_mon_810_leap_load_layer2/complex_leap_exp_20250414_003722/load_factors_by_term_depth3_10_leap_d40_h4096_depth3_lr0.05_b512_modemup_distbinary_exp1_loadfactor_layerwise.png\n",
      "\n",
      "Load Factor Statistics:\n",
      "                     count      mean           std       min       25%  \\\n",
      "Depth Layer   Term                                                       \n",
      "3     Layer 1 term0   19.0  0.999726  1.140646e-16  0.999726  0.999726   \n",
      "              term1   19.0  0.999637  0.000000e+00  0.999637  0.999637   \n",
      "              term2   19.0  0.999516  2.281291e-16  0.999516  0.999516   \n",
      "      Layer 2 term0   19.0  0.119287  2.015739e-01  0.073043  0.073043   \n",
      "              term1   19.0  0.347538  1.475581e-01  0.313686  0.313686   \n",
      "              term2   19.0  0.957584  8.730691e-05  0.957224  0.957604   \n",
      "      Layer 3 term0   19.0  0.100199  2.067950e-01  0.052163  0.052163   \n",
      "              term1   19.0  0.151032  1.955579e-01  0.105366  0.105366   \n",
      "              term2   19.0  0.957517  1.042272e-04  0.957087  0.957542   \n",
      "      Layer 4 term0   19.0  0.086239  2.102174e-01  0.037139  0.037139   \n",
      "              term1   19.0  0.088339  2.104318e-01  0.038748  0.038748   \n",
      "              term2   19.0  0.957498  4.165531e-04  0.957361  0.957361   \n",
      "\n",
      "                          50%       75%       max  \n",
      "Depth Layer   Term                                 \n",
      "3     Layer 1 term0  0.999726  0.999726  0.999726  \n",
      "              term1  0.999637  0.999637  0.999637  \n",
      "              term2  0.999516  0.999516  0.999516  \n",
      "      Layer 2 term0  0.073043  0.073043  0.951683  \n",
      "              term1  0.313686  0.313686  0.956876  \n",
      "              term2  0.957604  0.957604  0.957604  \n",
      "      Layer 3 term0  0.052163  0.052163  0.954095  \n",
      "              term1  0.105366  0.105366  0.958468  \n",
      "              term2  0.957542  0.957542  0.957542  \n",
      "      Layer 4 term0  0.037139  0.037139  0.954220  \n",
      "              term1  0.038748  0.038748  0.957073  \n",
      "              term2  0.957361  0.957361  0.958974  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import json\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "def extract_metadata_from_filename(filepath):\n",
    "    \"\"\"Extract metadata directly from filename when data extraction fails.\"\"\"\n",
    "    try:\n",
    "        base_filename = os.path.basename(filepath)\n",
    "        \n",
    "        # Handle _loadfactor_layerwise pattern\n",
    "        match = re.match(r'(\\w+)_d(\\d+)_h(\\d+)_depth(\\d+)_lr([\\d\\.]+)_b(\\d+)_mode(\\w+)_dist(\\w+)_exp(\\d+)(?:_loadfactor_layerwise)?', base_filename)\n",
    "        \n",
    "        if match:\n",
    "            func_name, dim, hidden, depth, lr, batch, mode, dist, exp = match.groups()\n",
    "            return {\n",
    "                'function_name': func_name,\n",
    "                'input_dim': int(dim),\n",
    "                'hidden_size': int(hidden),\n",
    "                'depth': int(depth),\n",
    "                'learning_rate': float(lr),\n",
    "                'batch_size': int(batch),\n",
    "                'mode': mode,\n",
    "                'input_distribution': dist,\n",
    "                'experiment_num': int(exp)\n",
    "            }\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metadata from filename {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_epoch_from_filename(filepath):\n",
    "    \"\"\"Extract epoch from the load factor filename, with improved pattern matching for layerwise variants.\"\"\"\n",
    "    try:\n",
    "        base_filename = os.path.basename(filepath)\n",
    "        \n",
    "        # Check for \"initial\" marker - various patterns\n",
    "        if \"_initial.npz\" in base_filename:\n",
    "            return 0\n",
    "            \n",
    "        # Check for \"final\" marker - various patterns\n",
    "        if \"_final.npz\" in base_filename:\n",
    "            try:\n",
    "                with np.load(filepath, allow_pickle=True) as data:\n",
    "                    if 'iteration' in data:\n",
    "                        return int(data['iteration'])\n",
    "            except:\n",
    "                pass\n",
    "            # Default for final iterations if not found in file\n",
    "            return 1000000\n",
    "            \n",
    "        # Try more specific patterns first, then fall back to more general ones\n",
    "        \n",
    "        # Pattern for layerwise termloadfactor files\n",
    "        layerwise_term_match = re.search(r'_layerwise_termloadfactor_iter(\\d+)', base_filename)\n",
    "        if layerwise_term_match:\n",
    "            return int(layerwise_term_match.group(1))\n",
    "            \n",
    "        # Pattern for layerwise loadfactor files\n",
    "        layerwise_match = re.search(r'_layerwise_loadfactor_iter(\\d+)', base_filename)\n",
    "        if layerwise_match:\n",
    "            return int(layerwise_match.group(1))\n",
    "            \n",
    "        # Pattern for regular loadfactor files\n",
    "        loadfactor_match = re.search(r'_loadfactor_iter(\\d+)', base_filename)\n",
    "        if loadfactor_match:\n",
    "            return int(loadfactor_match.group(1))\n",
    "            \n",
    "        # General pattern matching any iter number\n",
    "        general_match = re.search(r'_iter(\\d+)\\.npz$', base_filename)\n",
    "        if general_match:\n",
    "            return int(general_match.group(1))\n",
    "            \n",
    "        # If we get here, we couldn't determine the epoch\n",
    "        print(f\"Could not determine epoch for {base_filename}, skipping\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting epoch from {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_load_factors(file_path):\n",
    "    \"\"\"Load load factors from file with proper error handling.\"\"\"\n",
    "    try:\n",
    "        with np.load(file_path, allow_pickle=True) as data:\n",
    "            # Debug: print all keys in the file\n",
    "            print(f\"Keys in {os.path.basename(file_path)}: {list(data.keys())}\")\n",
    "            \n",
    "            if 'load_factors' in data:\n",
    "                load_factors = data['load_factors']\n",
    "                \n",
    "                # Check if load_factors is a dictionary\n",
    "                if isinstance(load_factors, np.ndarray) and load_factors.dtype == np.dtype('O'):\n",
    "                    # This is likely a dictionary stored as an object array\n",
    "                    load_factors = load_factors.item()\n",
    "                \n",
    "                if isinstance(load_factors, dict):\n",
    "                    return load_factors\n",
    "                else:\n",
    "                    print(f\"load_factors in {file_path} is not a dictionary: {type(load_factors)}\")\n",
    "            else:\n",
    "                print(f\"No load_factors in {file_path}\")\n",
    "                \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_training_data(folder_path, model_base):\n",
    "    \"\"\"\n",
    "    Load training data from either main results file or the loadfactor file.\n",
    "    Tries multiple filename patterns to handle different naming conventions.\n",
    "    Returns tuple of (epochs, losses) or None if not found.\n",
    "    \"\"\"\n",
    "    # Try several possible filenames\n",
    "    possible_filenames = [\n",
    "        f\"{model_base}.npz\",\n",
    "        f\"{model_base}_loadfactor.npz\",\n",
    "        model_base.replace(\"_loadfactor_layerwise\", \"\") + \".npz\",\n",
    "        model_base.replace(\"_loadfactor\", \"\") + \".npz\",\n",
    "        model_base.split(\"_loadfactor\")[0] + \".npz\" if \"_loadfactor\" in model_base else None,\n",
    "    ]\n",
    "    \n",
    "    # Filter out None values\n",
    "    possible_filenames = [f for f in possible_filenames if f]\n",
    "    \n",
    "    # Try all possible files\n",
    "    for filename in possible_filenames:\n",
    "        results_file = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if os.path.exists(results_file):\n",
    "            try:\n",
    "                print(f\"Attempting to load training data from: {results_file}\")\n",
    "                with np.load(results_file, allow_pickle=True) as data:\n",
    "                    if 'train_stats' in data:\n",
    "                        train_stats = data['train_stats']\n",
    "                        # Extract epochs and losses\n",
    "                        if len(train_stats) > 0:\n",
    "                            epochs = train_stats[:, 0]\n",
    "                            losses = train_stats[:, 1]\n",
    "                            print(f\"Successfully loaded training data from {results_file}: {len(epochs)} epochs\")\n",
    "                            return (epochs, losses)\n",
    "                        else:\n",
    "                            print(f\"Training data array is empty in {results_file}\")\n",
    "                    else:\n",
    "                        print(f\"No 'train_stats' key found in {results_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading training data from {results_file}: {e}\")\n",
    "    \n",
    "    # If we get here, we've tried all possible files and found nothing\n",
    "    print(f\"Could not find training data for {model_base} after trying multiple filename patterns\")\n",
    "    \n",
    "    # Last resort: try to find ALL .npz files and look for matches in their names\n",
    "    npz_files = glob(os.path.join(folder_path, \"*.npz\"))\n",
    "    for file in npz_files:\n",
    "        base_name = os.path.basename(file)\n",
    "        # Look for files that share the most significant part of the name\n",
    "        if model_base.split(\"_loadfactor\")[0] in base_name and \"_termloadfactor_\" not in base_name:\n",
    "            try:\n",
    "                print(f\"Trying alternative file: {file}\")\n",
    "                with np.load(file, allow_pickle=True) as data:\n",
    "                    if 'train_stats' in data:\n",
    "                        train_stats = data['train_stats']\n",
    "                        # Extract epochs and losses\n",
    "                        if len(train_stats) > 0:\n",
    "                            epochs = train_stats[:, 0]\n",
    "                            losses = train_stats[:, 1]\n",
    "                            print(f\"Successfully loaded training data from {file}: {len(epochs)} epochs\")\n",
    "                            return (epochs, losses)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading training data from alternative file {file}: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def analyze_layerwise_termloadfactor_file(filepath):\n",
    "    \"\"\"\n",
    "    Extract data specifically from layerwise_termloadfactor files.\n",
    "    Returns a list of dictionaries in the same format as analyze_load_factors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with np.load(filepath, allow_pickle=True) as data:\n",
    "            # Debug output\n",
    "            print(f\"Keys in {os.path.basename(filepath)}: {list(data.keys())}\")\n",
    "            \n",
    "            # Check for required keys\n",
    "            if not all(key in data for key in ['load_factors', 'term_descriptions']):\n",
    "                print(f\"Missing required keys in {filepath}\")\n",
    "                return []\n",
    "                \n",
    "            # Extract epoch from filename\n",
    "            epoch = extract_epoch_from_filename(filepath)\n",
    "            if epoch is None:\n",
    "                # Try to get it from metadata\n",
    "                if 'metadata' in data and len(data['metadata']) > 0:\n",
    "                    try:\n",
    "                        metadata_str = data['metadata'][0]\n",
    "                        metadata_dict = eval(metadata_str)\n",
    "                        epoch = metadata_dict.get('iteration', None)\n",
    "                        print(f\"Extracted epoch {epoch} from metadata\")\n",
    "                    except:\n",
    "                        print(f\"Failed to parse metadata in {filepath}\")\n",
    "                        \n",
    "            if epoch is None:\n",
    "                print(f\"Could not determine epoch for {filepath}, skipping\")\n",
    "                return []\n",
    "                \n",
    "            # Extract base model info from filename\n",
    "            model_base = os.path.basename(filepath)\n",
    "            # Remove the termloadfactor part\n",
    "            model_base = re.sub(r'_termloadfactor_(?:initial|iter\\d+|final)\\.npz$', '', model_base)\n",
    "            \n",
    "            # Extract metadata from filename\n",
    "            metadata = extract_metadata_from_filename(filepath)\n",
    "            if not metadata:\n",
    "                print(f\"Could not extract metadata from {filepath}, skipping\")\n",
    "                return []\n",
    "                \n",
    "            depth = metadata.get('depth', 0)\n",
    "            \n",
    "            # Get load factors\n",
    "            load_factors = data['load_factors'].item() if data['load_factors'].dtype == np.dtype('O') else data['load_factors']\n",
    "            term_descriptions = data['term_descriptions']\n",
    "            \n",
    "            # Process load factors\n",
    "            all_data = []\n",
    "            for term_key, layer_factors in load_factors.items():\n",
    "                # Create layer names dynamically based on actual number of layers\n",
    "                num_layers = len(layer_factors)\n",
    "                layers = [f\"Layer {i+1}\" for i in range(num_layers)]\n",
    "                \n",
    "                for layer_idx, factor in enumerate(layer_factors):\n",
    "                    if layer_idx < len(layers):  # Ensure we don't go out of bounds\n",
    "                        all_data.append({\n",
    "                            'Epoch': epoch,\n",
    "                            'Layer': layers[layer_idx],\n",
    "                            'Term': term_key,\n",
    "                            'Load Factor': factor,\n",
    "                            'Model': model_base,\n",
    "                            'Depth': depth\n",
    "                        })\n",
    "            \n",
    "            return all_data\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing layerwise termloadfactor file {filepath}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def analyze_load_factors(folder_path):\n",
    "    \"\"\"Analyze load factor files in a given folder with improved error handling for layerwise variants.\"\"\"\n",
    "    # Find termloadfactor files specifically\n",
    "    termloadfactor_files = glob(os.path.join(folder_path, \"*termloadfactor*.npz\"))\n",
    "    print(f\"Found {len(termloadfactor_files)} termloadfactor files\")\n",
    "    \n",
    "    # Process all termloadfactor files directly\n",
    "    all_data = []\n",
    "    for file_path in termloadfactor_files:\n",
    "        file_data = analyze_layerwise_termloadfactor_file(file_path)\n",
    "        all_data.extend(file_data)\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No data was successfully processed from termloadfactor files. Trying standard load factor files...\")\n",
    "        \n",
    "        # Find all standard load factor files that match the pattern\n",
    "        load_factor_files = glob(os.path.join(folder_path, \"*loadfactor*iter*.npz\"))\n",
    "        load_factor_files += glob(os.path.join(folder_path, \"*loadfactor*initial.npz\"))\n",
    "        load_factor_files += glob(os.path.join(folder_path, \"*loadfactor*final.npz\"))\n",
    "        load_factor_files = [f for f in load_factor_files if \"termloadfactor\" not in f]  # Exclude termloadfactor files\n",
    "        print(f\"Found {len(load_factor_files)} standard load factor files\")\n",
    "        \n",
    "        # Group files by model configuration\n",
    "        model_files = {}\n",
    "        for file_path in load_factor_files:\n",
    "            base_filename = os.path.basename(file_path)\n",
    "            \n",
    "            # Extract the base model name without the epoch/iteration info\n",
    "            # Handle the double loadfactor case\n",
    "            model_base = re.sub(r'_loadfactor_(?:layerwise_)?loadfactor_(?:initial|iter\\d+|final)\\.npz$', '', base_filename)\n",
    "            model_base = re.sub(r'_loadfactor_(?:initial|iter\\d+|final)\\.npz$', '', model_base)\n",
    "            \n",
    "            if model_base not in model_files:\n",
    "                model_files[model_base] = []\n",
    "            \n",
    "            # Extract the epoch\n",
    "            epoch = extract_epoch_from_filename(file_path)\n",
    "            if epoch is not None:\n",
    "                model_files[model_base].append((epoch, file_path))\n",
    "        \n",
    "        # Sort files by epoch for each model\n",
    "        for model_base in model_files:\n",
    "            model_files[model_base].sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Process each model's load factor data\n",
    "        standard_data = []\n",
    "        \n",
    "        for model_base, file_list in model_files.items():\n",
    "            if not file_list:\n",
    "                print(f\"No valid files for {model_base}, skipping\")\n",
    "                continue\n",
    "                \n",
    "            metadata = None\n",
    "            \n",
    "            # Try to get metadata from filename\n",
    "            metadata = extract_metadata_from_filename(file_list[0][1])\n",
    "            \n",
    "            if not metadata:\n",
    "                print(f\"Could not extract metadata for {model_base}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            depth = metadata.get('depth', 0)\n",
    "            \n",
    "            print(f\"Processing model: {model_base}, depth: {depth}\")\n",
    "            \n",
    "            for epoch, file_path in file_list:\n",
    "                # Load load factors using the improved function\n",
    "                load_factors = load_load_factors(file_path)\n",
    "                \n",
    "                if not load_factors:\n",
    "                    print(f\"Failed to load factors from {file_path}, skipping\")\n",
    "                    continue\n",
    "                    \n",
    "                # Get the number of layers from the first term\n",
    "                first_term = next(iter(load_factors))\n",
    "                factors_for_first_term = load_factors[first_term]\n",
    "                \n",
    "                if isinstance(factors_for_first_term, list):\n",
    "                    num_layers = len(factors_for_first_term)\n",
    "                    \n",
    "                    # Create layer names dynamically based on actual number of layers\n",
    "                    layers = [f\"Layer {i+1}\" for i in range(num_layers)]\n",
    "                    \n",
    "                    # Process each term's load factors\n",
    "                    for term in load_factors:\n",
    "                        factors = load_factors[term]\n",
    "                        for layer_idx, factor in enumerate(factors):\n",
    "                            if layer_idx < len(layers):  # Ensure we don't go out of bounds\n",
    "                                standard_data.append({\n",
    "                                    'Epoch': epoch,\n",
    "                                    'Layer': layers[layer_idx],\n",
    "                                    'Term': term,\n",
    "                                    'Load Factor': factor,\n",
    "                                    'Model': model_base,\n",
    "                                    'Depth': depth\n",
    "                                })\n",
    "                else:\n",
    "                    print(f\"Unexpected format for factors: {factors_for_first_term}\")\n",
    "        \n",
    "        # Combine with termloadfactor data\n",
    "        if standard_data:\n",
    "            df = pd.DataFrame(standard_data)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No data was successfully processed. Dataframe is empty.\")\n",
    "        return df\n",
    "    \n",
    "    # Now create plots\n",
    "    print(f\"Successfully processed data for {len(df['Model'].unique())} models\")\n",
    "    plot_load_factors_by_depth(df, folder_path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_load_factors_by_depth(df, output_folder):\n",
    "    \"\"\"\n",
    "    Create separate plots for each depth with training error included.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with load factor data\n",
    "        output_folder: Folder to save the plots\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"DataFrame is empty, cannot create plots.\")\n",
    "        return\n",
    "    \n",
    "    # Get unique depths\n",
    "    depths = df['Depth'].unique()\n",
    "    \n",
    "    for depth in depths:\n",
    "        depth_df = df[df['Depth'] == depth]\n",
    "        \n",
    "        # Get unique models for this depth\n",
    "        models = depth_df['Model'].unique()\n",
    "        \n",
    "        for model in models:\n",
    "            model_df = depth_df[depth_df['Model'] == model]\n",
    "            \n",
    "            # Get unique terms\n",
    "            terms = model_df['Term'].unique()\n",
    "            \n",
    "            # Get unique layers\n",
    "            layers = model_df['Layer'].unique()\n",
    "            num_layers = len(layers)\n",
    "            \n",
    "            # Create figure with num_layers + 1 rows (layers + training error)\n",
    "            fig, axes = plt.subplots(num_layers + 1, 1, figsize=(12, 3 * (num_layers + 1)), sharex=True)\n",
    "            \n",
    "            # Handle the case where there's only one subplot\n",
    "            if num_layers == 0:\n",
    "                print(f\"No layers found for model {model}\")\n",
    "                continue\n",
    "                \n",
    "            # Make axes a list if it's a single subplot\n",
    "            if num_layers == 1:\n",
    "                axes = [axes]\n",
    "            elif not isinstance(axes, list):\n",
    "                axes = axes.flatten()\n",
    "            \n",
    "            # Plot load factors for each layer\n",
    "            for i, layer in enumerate(sorted(layers, key=lambda x: int(x.split()[1]))):\n",
    "                layer_df = model_df[model_df['Layer'] == layer]\n",
    "                \n",
    "                for j, term in enumerate(sorted(terms)):\n",
    "                    term_df = layer_df[layer_df['Term'] == term]\n",
    "                    if not term_df.empty:\n",
    "                        # Use a different color for each term and add transparency\n",
    "                        color = plt.cm.tab10(j % 10)\n",
    "                        axes[i].plot(\n",
    "                            term_df['Epoch'], \n",
    "                            term_df['Load Factor'], \n",
    "                            'o-', \n",
    "                            label=term,\n",
    "                            color=color,\n",
    "                            alpha=0.7,  # Add transparency\n",
    "                            linewidth=2,\n",
    "                            markersize=4\n",
    "                        )\n",
    "                \n",
    "                axes[i].set_ylabel('Load Factor (RMSE)')\n",
    "                axes[i].set_title(f\"{layer}\")\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                axes[i].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "                axes[i].set_xscale('log')  # Use log scale for x-axis\n",
    "            \n",
    "            # Load and plot training error in the last subplot\n",
    "            training_data = load_training_data(output_folder, model)\n",
    "            \n",
    "            if training_data:\n",
    "                epochs, losses = training_data\n",
    "                axes[-1].plot(epochs, losses, 'r-', label='Training Loss', linewidth=2)\n",
    "                axes[-1].set_ylabel('Training Loss (MSE)')\n",
    "                axes[-1].set_yscale('log')  # Log scale for training loss\n",
    "                axes[-1].grid(True, alpha=0.3)\n",
    "                axes[-1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "            else:\n",
    "                axes[-1].text(0.5, 0.5, 'No training data available', \n",
    "                             horizontalalignment='center', verticalalignment='center',\n",
    "                             transform=axes[-1].transAxes)\n",
    "            \n",
    "            # Set common x-label\n",
    "            axes[-1].set_xlabel('Epoch')\n",
    "            axes[-1].set_xscale('log')  # Use log scale for x-axis\n",
    "            \n",
    "            # Add title\n",
    "            fig_title = f\"Load Factor Evolution by Layer and Term Combination\\nModel: {model}, Depth: {depth}\"\n",
    "            plt.suptitle(fig_title, fontsize=16)\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for title\n",
    "            \n",
    "            # Save figure\n",
    "            output_path = os.path.join(output_folder, f\"load_factors_depth{depth}_{model}.png\")\n",
    "            plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"Saved plot to {output_path}\")\n",
    "\n",
    "\n",
    "def plot_separate_terms(df, output_folder):\n",
    "    \"\"\"\n",
    "    Create plots with each term in a separate panel for better visibility.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with load factor data\n",
    "        output_folder: Folder to save the plots\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"DataFrame is empty, cannot create plots.\")\n",
    "        return\n",
    "    \n",
    "    # Get unique depths\n",
    "    depths = df['Depth'].unique()\n",
    "    \n",
    "    for depth in depths:\n",
    "        depth_df = df[df['Depth'] == depth]\n",
    "        \n",
    "        # Get unique models for this depth\n",
    "        models = depth_df['Model'].unique()\n",
    "        \n",
    "        for model in models:\n",
    "            model_df = depth_df[depth_df['Model'] == model]\n",
    "            \n",
    "            # Get unique terms and layers\n",
    "            terms = sorted(model_df['Term'].unique())\n",
    "            layers = sorted(model_df['Layer'].unique(), key=lambda x: int(x.split()[1]))\n",
    "            \n",
    "            # Create a figure with terms as rows and layers as columns\n",
    "            fig, axes = plt.subplots(len(terms), 1, figsize=(12, 4 * len(terms)), sharex=True)\n",
    "            \n",
    "            # Make axes a list if it's a single subplot\n",
    "            if len(terms) == 1:\n",
    "                axes = [axes]\n",
    "            elif not isinstance(axes, list):\n",
    "                axes = axes.flatten()\n",
    "            \n",
    "            # Load training data\n",
    "            training_data = load_training_data(output_folder, model)\n",
    "            \n",
    "            # Plot each term in a separate row\n",
    "            for i, term in enumerate(terms):\n",
    "                term_df = model_df[model_df['Term'] == term]\n",
    "                \n",
    "                # Plot all layers for this term on the same subplot\n",
    "                for j, layer in enumerate(layers):\n",
    "                    layer_data = term_df[term_df['Layer'] == layer]\n",
    "                    if not layer_data.empty:\n",
    "                        # Use a different color for each layer\n",
    "                        color = plt.cm.viridis(j / max(1, len(layers) - 1))\n",
    "                        axes[i].plot(\n",
    "                            layer_data['Epoch'],\n",
    "                            layer_data['Load Factor'],\n",
    "                            'o-', \n",
    "                            label=layer,\n",
    "                            color=color,\n",
    "                            linewidth=2,\n",
    "                            markersize=4\n",
    "                        )\n",
    "                \n",
    "                # Add training loss to each term subplot as a dotted line\n",
    "                if training_data:\n",
    "                    epochs, losses = training_data\n",
    "                    # Use a twin axis for training loss\n",
    "                    ax2 = axes[i].twinx()\n",
    "                    ax2.plot(epochs, losses, 'r--', label='Training Loss', alpha=0.5, linewidth=1.5)\n",
    "                    ax2.set_ylabel('Training Loss (MSE)', color='r')\n",
    "                    ax2.set_yscale('log')\n",
    "                    ax2.tick_params(axis='y', labelcolor='r')\n",
    "                    \n",
    "                axes[i].set_ylabel('Load Factor (RMSE)')\n",
    "                axes[i].set_title(f\"Term: {term}\")\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                axes[i].legend(loc='upper left')\n",
    "                axes[i].set_xscale('log')\n",
    "            \n",
    "            # Set common x-label on the bottom subplot\n",
    "            axes[-1].set_xlabel('Epoch')\n",
    "            \n",
    "            # Add title\n",
    "            fig_title = f\"Load Factor Evolution by Term\\nModel: {model}, Depth: {depth}\"\n",
    "            plt.suptitle(fig_title, fontsize=16)\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for title\n",
    "            \n",
    "            # Save figure\n",
    "            output_path = os.path.join(output_folder, f\"load_factors_by_term_depth{depth}_{model}.png\")\n",
    "            plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"Saved plot to {output_path}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Modify this path to your results folder\n",
    "    folder_path = \"/home/goring/OnlineSGD/results_MSP/Phi_1304_mon_810_leap_load_layer2/complex_leap_exp_20250414_003722\"\n",
    "    \n",
    "    # Analyze and create standard plots\n",
    "    df = analyze_load_factors(folder_path)\n",
    "    \n",
    "    # Create additional plots with terms separated for better visibility\n",
    "    if not df.empty:\n",
    "        print(f\"Successfully processed {len(df)} data points\")\n",
    "        plot_separate_terms(df, folder_path)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nLoad Factor Statistics:\")\n",
    "        print(df.groupby(['Depth', 'Layer', 'Term'])['Load Factor'].describe())\n",
    "    else:\n",
    "        print(\"No data was successfully processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_jax_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
