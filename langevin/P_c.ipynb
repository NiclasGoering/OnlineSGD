{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00129b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import factorial\n",
    "from scipy.misc import derivative\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Define Model Hyperparameters ---\n",
    "# These can be changed to explore different scenarios.\n",
    "params = {\n",
    "    \"d\": 20,          # Input dimension\n",
    "    \"k_parity\": 2,    # Sparsity of the target parity function (k)\n",
    "    \"N\": 1000,         # Number of neurons in the hidden layer\n",
    "    \"gamma\": 0.0,     # Scaling exponent for the output weights' prior\n",
    "    \"kappa\": 0.05,     # Noise parameter in the loss function\n",
    "    \"sigma_v\": 1.0,   # Variance parameter for input weights w\n",
    "    \"sigma_w\": 1.0,   # Variance parameter for output weights a\n",
    "}\n",
    "\n",
    "\n",
    "# --- 2. Special Handling for ReLU Taylor Coefficients ---\n",
    "# The pure ReLU function is not smooth at z=0, so its Taylor series\n",
    "# is ill-defined. In theoretical models, one typically considers a\n",
    "# smoothed version. Here, we use numerical differentiation on a\n",
    "# slightly smoothed ReLU to find the coefficients c_k = phi^(k)(0) / k!\n",
    "\n",
    "def smoothed_relu(z, delta=1e-4):\n",
    "    \"\"\"A smooth approximation of the ReLU function.\"\"\"\n",
    "    return (z + np.sqrt(z**2 + delta**2)) / 2\n",
    "\n",
    "def get_relu_taylor_coeff(k):\n",
    "    \"\"\"\n",
    "    Numerically calculates the k-th Taylor coefficient of the smoothed ReLU.\n",
    "    c_k = phi^(k)(0) / k!\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return 0.0 # ReLU(0) = 0\n",
    "    \n",
    "    # Use numerical differentiation to find the k-th derivative at z=0\n",
    "    # dx=1e-2 is a good step size for numerical stability here.\n",
    "    k_th_derivative_at_0 = derivative(smoothed_relu, 0.0, n=k, dx=1e-2, order=k+1+(k%2))\n",
    "    \n",
    "    return k_th_derivative_at_0 / factorial(k)\n",
    "\n",
    "# --- 3. Method 1: Analytical Calculation of P_c ---\n",
    "\n",
    "def calculate_Pc_analytical(params):\n",
    "    \"\"\"\n",
    "    Calculates P_c using the analytical formulas for the average quantities.\n",
    "    \"\"\"\n",
    "    d = params[\"d\"]\n",
    "    k = params[\"k_parity\"]\n",
    "    N = params[\"N\"]\n",
    "    gamma = params[\"gamma\"]\n",
    "    kappa = params[\"kappa\"]\n",
    "    sigma_v = params[\"sigma_v\"]\n",
    "    sigma_w = params[\"sigma_w\"]\n",
    "\n",
    "    # Get the necessary Taylor coefficient for the target parity k\n",
    "    c_k = get_relu_taylor_coeff(k)\n",
    "    if np.abs(c_k) < 1e-9:\n",
    "        print(f\"Warning: Taylor coefficient c_{k} is nearly zero. \"\n",
    "              \"The model may not be able to learn this parity. P_c will be infinite.\")\n",
    "        return np.inf\n",
    "        \n",
    "    # a) Calculate average self-energy <Sigma>\n",
    "    # For ReLU, the constant alpha_phi is 1/2.\n",
    "    alpha_phi = 0.5 \n",
    "    avg_Sigma = alpha_phi * sigma_v**2\n",
    "    \n",
    "    # b) Calculate average squared projection onto the target <J_S^2>\n",
    "    avg_J_S_sq = (c_k * factorial(k))**2 * (sigma_v**2 / d)**k\n",
    "    \n",
    "    # c) Calculate average interference noise <Upsilon> using the approximation\n",
    "    avg_Upsilon = avg_Sigma - avg_J_S_sq\n",
    "    \n",
    "    # d) Calculate P_c using the final derived formula\n",
    "    # P_c = <Υ> / [ (κ*N^γ/σ_w²) + <Σ> - N*<J_S²> ]\n",
    "    \n",
    "    # Note: This is one of the possible final formulas. Different conventions in the\n",
    "    # base action can change the powers of kappa. We use a self-consistent one here.\n",
    "    denominator = (kappa * N**gamma / sigma_w**2) + avg_Sigma - N * avg_J_S_sq\n",
    "    \n",
    "    if denominator <= 0:\n",
    "        print(\"Learning is always favorable or at a critical point (denominator <= 0). P_c is effectively 0.\")\n",
    "        return 0\n",
    "        \n",
    "    P_c = avg_Upsilon / denominator\n",
    "    \n",
    "    return P_c\n",
    "\n",
    "\n",
    "# --- 4. Method 2: Monte Carlo Estimation of P_c ---\n",
    "\n",
    "def calculate_Pc_monte_carlo(params, num_w_samples=10000, num_x_samples=500):\n",
    "    \"\"\"\n",
    "    Estimates P_c by sampling random weight vectors `w` and inputs `x`.\n",
    "    \"\"\"\n",
    "    d = params[\"d\"]\n",
    "    k = params[\"k_parity\"]\n",
    "    N = params[\"N\"]\n",
    "    gamma = params[\"gamma\"]\n",
    "    kappa = params[\"kappa\"]\n",
    "    sigma_v = params[\"sigma_v\"]\n",
    "    sigma_w = params[\"sigma_w\"]\n",
    "\n",
    "    # Accumulators for our averages\n",
    "    acc_Sigma = 0.0\n",
    "    acc_J_S_sq = 0.0\n",
    "    \n",
    "    # Generate a fixed sparse set S for the target parity\n",
    "    S_indices = np.random.choice(d, k, replace=False)\n",
    "\n",
    "    print(\"Running Monte Carlo simulation... (this may take a moment)\")\n",
    "    for _ in range(num_w_samples):\n",
    "        # Sample a random weight vector w\n",
    "        w = np.random.randn(d) * (sigma_v / np.sqrt(d))\n",
    "        \n",
    "        # Estimate Sigma(w) = 0.5 * ||w||^2\n",
    "        Sigma_w = 0.5 * np.sum(w**2)\n",
    "        acc_Sigma += Sigma_w\n",
    "        \n",
    "        # Estimate J_S(w) = E_x[ReLU(w^T*x) * chi_S(x)] via sampling x\n",
    "        # This is a nested Monte Carlo step\n",
    "        acc_J_S_w = 0.0\n",
    "        for _ in range(num_x_samples):\n",
    "            x = np.random.choice([-1, 1], size=d)\n",
    "            z = np.dot(w, x)\n",
    "            relu_z = np.maximum(0, z)\n",
    "            chi_S = np.prod(x[S_indices])\n",
    "            acc_J_S_w += relu_z * chi_S\n",
    "        \n",
    "        J_S_w = acc_J_S_w / num_x_samples\n",
    "        acc_J_S_sq += J_S_w**2\n",
    "        \n",
    "    # Finalize the averages\n",
    "    avg_Sigma_mc = acc_Sigma / num_w_samples\n",
    "    avg_J_S_sq_mc = acc_J_S_sq / num_w_samples\n",
    "    \n",
    "    # Calculate Upsilon from the estimated averages\n",
    "    avg_Upsilon_mc = avg_Sigma_mc - avg_J_S_sq_mc\n",
    "    \n",
    "    # Calculate P_c using the same final formula as the analytical method\n",
    "    denominator = (kappa * N**gamma / sigma_w**2) + avg_Sigma_mc - N * avg_J_S_sq_mc\n",
    "    \n",
    "    if denominator <= 0:\n",
    "        print(\"MC Estimation: Learning is always favorable (denominator <= 0). P_c is effectively 0.\")\n",
    "        return 0\n",
    "        \n",
    "    P_c_mc = avg_Upsilon_mc / denominator\n",
    "    \n",
    "    return P_c_mc\n",
    "\n",
    "\n",
    "# --- 5. Main Execution ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- Calculating Critical Dataset Size P_c ---\\n\")\n",
    "    print(f\"Hyperparameters: {params}\\n\")\n",
    "\n",
    "    # Analytical result\n",
    "    print(\"Method 1: Using Analytical Approximations\")\n",
    "    P_c_analytic = calculate_Pc_analytical(params)\n",
    "    print(f\"  --> Analytical P_c = {P_c_analytic:.2f}\\n\")\n",
    "\n",
    "    # Monte Carlo result\n",
    "    print(\"Method 2: Using Monte Carlo Estimation\")\n",
    "    P_c_mc = calculate_Pc_monte_carlo(params)\n",
    "    print(f\"  --> Monte Carlo P_c = {P_c_mc:.2f}\\n\")\n",
    "    \n",
    "    print(\"Note: The two results should be close. Differences arise from MC sampling error and approximations in the analytical formulas.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
